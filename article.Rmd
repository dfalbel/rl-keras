---
title: "Playing Atari with Keras and R"
author: "Daniel Falbel and Fernando Barscevicius"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this article we will implement the famous paper from DeepMind: [*Playing Atari with Deep Reinforcement Learning*](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) in R using Keras. This paper introduced the first deep learning model that successfully learned control policies directly from high-dimensional sensory input using reinforcement learning.

The DeepMinds's algorithm is trained on raw pixels from Atari games and estimates future rewards for each possible action. The model is a convolutional neural network trained with a variant of the [Q-Learning](https://en.wikipedia.org/wiki/Q-learning) algorithm, which we will attempt to explain during this article.

The code was not published with the paper, but the results were reproduced and published by severeal people on the internet. Particularly, [this post](https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html) and the [referenced implementation](https://github.com/yenchenlin/DeepLearningFlappyBird) were very useful when writing this article. Also, [this article](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26) from [Becoming Human](https://becominghuman.ai) was very inspiring.

We only tested our implementation for the Atari Breakout game, but it should also work for other Atari games.

# Reinforcement Learning

Reinforcement learning is an area of Machine Learning that has been gaining a lot of attention recently. A good definition comes from Sutton and Barto's book *Reinforcement Learning: An Introduction*:

> Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.

Reinforcement learning is being used to solve many problems in robotics, delivery management or in the finance sector.

## Q-Learning

Q-learning, [as defined by it's creator](https://link.springer.com/content/pdf/10.1007%2FBF00992698.pdf), *is a form of model-free reinforcement learning. It can also be viewed as a method of asynchronous dynamic programming (DP). It provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains.*

Put in simple terms, Q-learning is an algorithm that allows for the selection of the best possible action to perform given the current state of the environment. It works by assigning each state an estimated value **(Q-value)**. Then, by interacting with the state and receiveing a reward, the estimated value is updated. We can write this Q-value as $\mathbf{Q(s, a)}$ and the updates are given by $\mathbf{Q(s, a) = Q(s, a) + α(R(s) + γmax_{a'}Q(s_0, a_0) − Q(s, a))}$

## Deep Q-Learning

# Implementation in R

## Gym












