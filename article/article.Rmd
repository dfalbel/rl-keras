---
title: "Playing Atari Breakout with Keras and R"
description: |
  In this article we will implement the famous paper from DeepMind: *Playing Atari 
  with Deep Reinforcement Learning* in R using Keras. This paper introduced the 
  first deep learning model that successfully learned control policies directly 
  from high-dimensional sensory input using reinforcement learning.
author:
  - name: Daniel Falbel 
    url: https://github.com/dfalbel
    affiliation: Curso-R
    affiliation_url: http://curso-r.com
  - name: Fernando Barscevicius
    url: https://github.com/feebarscevicius
    affiliation: Marketdata
    affiliation_url: http://marketdata.com.br
date: 07-17-2018
creative_commons: CC BY
repository_url: https://github.com/dfalbel/rl-keras
output: 
  radix::radix_article:
    self_contained: false
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

In this article we will implement the famous paper from DeepMind: [*Playing Atari with Deep Reinforcement Learning*](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) in R using Keras. This paper introduced the first deep learning model that successfully learned control policies directly from high-dimensional sensory input using reinforcement learning.

The DeepMinds's algorithm is trained on raw pixels from Atari games and estimates future rewards for each possible action. The model is a convolutional neural network trained with a variant of the [Q-Learning](https://en.wikipedia.org/wiki/Q-learning) algorithm, which we will attempt to explain during this article.

The code was not published with the paper, but the results were reproduced and published by severeal people on the internet. Particularly, [this post](https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html) and the [referenced implementation](https://github.com/yenchenlin/DeepLearningFlappyBird) were very useful when writing this article. Also, [this article](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26) from [Becoming Human](https://becominghuman.ai) was very inspiring.

We only tested our implementation for the Atari Breakout game, but it should also work for other Atari games.

# Reinforcement Learning

Reinforcement learning is an area of Machine Learning that has been gaining a lot of attention recently. A good definition comes from Sutton and Barto's book *Reinforcement Learning: An Introduction*:

> Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.

Reinforcement learning is being used to solve many problems in robotics, delivery management or in the finance sector.

## Q-Learning

Q-learning, [as defined by it's creator](https://link.springer.com/content/pdf/10.1007%2FBF00992698.pdf), *is a form of model-free reinforcement learning. It can also be viewed as a method of asynchronous dynamic programming (DP). It provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains.*

Put in simple terms, Q-learning is an algorithm that allows for the selection of the best possible action to perform given the current state of the environment. It works by assigning to each state an estimated value **(Q-value)**. Then, by interacting with the state and receiveing a reward, the estimated value is updated. We can write this Q-value as $\mathbf{Q(s, a)}$ and the updates are given by: $$\mathbf{Q(s, a) = Q(s, a) + \alpha(R(s) + γmax_{a'}Q(s_0, a_0) − Q(s, a))}$$ 

<aside>
More information about the mathematics and the workings of the Q-learning algorithm can be found [here](http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/).
</aside>

As we have to keep track of the Q-values for every state, storing and updating them in a Q-table, Q-learning can become very ineffective for bigger environments. That's where Deep Q-Learning comes into play, substituting the enormous Q-table for a lighter Neural Network.

## Deep Q-Learning

In Deep Q-Learning, the Q-table is substituted by a Neural Network mapping the current state to one action. This network approximates the reward based on the state.

![[Schematic illustration of the convolutional neural network](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)](dqn.png)

This model has one output for each possible action which represents the Q-value.

### Frame stacking

![[Frame Stacking](https://web.stanford.edu/class/cs221/2017/restricted/p-final/vishakh/final.pdf)](stack.png)

There is a problem with feeding the network with a single slice of the state in time: how can it infer motion? How can it take decisions without knowing the direction and speed of the objects in the frame? To solve this problem, we use a preprocessing technique called **frame stacking:** *n* number of frames (usually 4) are stacked together along a time dimension and fed to the model. This allows for the exploitation of spatial and temporal relationships for each step of the algorithm.

### Experience Replay

There are other problems with DQN: *correlation* and *forgetting*. As each gameplay has higly-correlated sequences of states, the network can be guided to a *local-minima*, which means it ends up learning just to replay the episode. It also tends to override past experiences when updating the weights as new episodes are played. To solve both, we implement the **Experience Replay**: every episode is stored in a memory buffer, and instead of updating the model's weights as the episode goes, the training takes place after the episode ends by randomly selecting sample batches from this memory buffer. As the buffer has information from lots of previous episodes and the sampling breaks the correlation between states of the same episode, both problems are gone!

### Algorithm
<pre>
<b>Deep Q Learning with Experience Replay</b>

Initialize replay memory D to size N
Initialize action-value function Q with random weights
<b>for</b> episode = 1, M <b>do</b>
  Initialize state s<SUB>1</SUB>
  <b>for</b> t = 1, T <b>do</b>
    With probability λ select random action a<SUB>t</SUB>
    Otherwise select a<SUB>t</SUB> = argmax<SUB>a</SUB>Q(s<SUB>t</SUB>, a; θ<SUB>i</SUB>)
    Execute action a<SUB>t</SUB> in emulator and observe reward r<SUB>t</SUB> and image x<SUB>t+1</SUB>
    Set s<SUB>t+1</SUB> = {s<SUB>t</SUB>, a<SUB>t</SUB>, x<SUB>t+1</SUB>} and preprocess Φ<SUB>t+1</SUB> = Φ(s<SUB>t</SUB> + 1)
    Store transition (s<SUB>t</SUB>, a<SUB>t</SUB>, r<SUB>t</SUB>, s<SUB>t+1</SUB>) in D
      Set y<SUB>j</SUB>:= r<SUB>j</SUB> for terminal s<SUB>j+1</SUB>
      Set y<SUB>j</SUB>:= r<SUB>j</SUB> + γ max a<SUB>0</SUB>Q(Φ<SUB>j+1</SUB>, a<SUB>0</SUB>; θ) for non terminal φ<SUB>j+1</SUB>
    Sample a minibatch of transitions (s<SUB>j</SUB>, a<SUB>j</SUB>, r<SUB>j</SUB>, s<SUB>j+1</SUB>) from D
    Perform a gradient step on (y<SUB>j</SUB> − Q(s<SUB>j</SUB>, a<SUB>j</SUB>; θ<SUB>i</SUB>))<SUP>2</SUP> with respect to θ
  <b>return</b>
<b>return</b>

</pre>

# Implementation in R

Now we will start implementing the Deep Q-Learning algorithm in R. But first, we need a game emulator. That's where [OpenAI gym](https://gym.openai.com) comes handy. 

## Gym

> Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.

The `gym` framework is available to python and we will use reticulate to run it from R. To install it, you can run:

```{r}
library(reticulate)
py_install("gym[atari]", envname = "r-tensorflow")
```

You must be careful to install it to the same environment that Keras and TensorFlow are installed since we will need to load all of them in the same session. Gym has lot's of dependencies that are not installed by default, to install the Atari dependencies that we are going to use we used `gym[atari]` in the pip package name.

Now that we have Gym installed we can "make" an environment and run some random actions to understand how it works.
We are going to use an environment called BreakoutDeterministic-v4. It differs from the canonical Breakout-v0 in two ways related noise added when performing actions:

* It doesn't repeat the performed action for more randomly chosen number of frames.
* It doesn't randomly repeat the previous action.

These differences make BreakoutDeterministic-v4 easier to learn than Breakout-v0.

```{r}
gym <- import("gym")
env <- gym$make("BreakoutDeterministic-v4")
initial_state <- env$reset()
```

Calling `env$reset()` will return a 3d array with the initial frame of the game.
We can render it by calliing `env$render()`.

With Gym we can also perform actions in the game by using the method `step`. We use the argument `action` to tell the environment which action to perform. Each different action is represented by an integer value. 

```{r}
result <- env$step(action = 0L)
```

The output of running a `step` in the environment is the next state of the game. The sate is represented by a list with the following structure:

```
List of 4
 $ : int [1:210, 1:160, 1:3] 0 0 0 0 0 0 0 0 0 0 ...
 $ : num 0
 $ : logi FALSE
 $ :List of 1
  ..$ ale.lives: int 5
```

The first element is an RGB array of the next frame, the second element is the reward received by performing the action and the thir is a boolean value indicating if the game has ended. The fourth element can contain other informations about the game, in this case, it returns the number of lives.

We can get the meaning of each action with `env.unwrapped.get_action_meanings()`. In our particular case, ie. the Atari Breakout game, there are 4 possible actions and the following table maps actions to corresponding integer values.

```{r, eval=TRUE, echo=FALSE}
knitr::kable(data.frame(
  "Action" = c("NOOP (Do nothing)", "FIRE (Get the ball)", "RIGHT", "LEFT" ),
  "Integer" = c(0, 1, 2, 3)
))
```

## Pre-processing

Now that we know about Gym, let's define some functions to preprocess the state before saving it to our experience memory. We will transform each frame to grayscale since the colors doesn't matter in this game and then we will rescale the image to save RAM memory.

We implemented 3 functions to do this:

```{r}
# transforms the input image to grayscale
grayscale <- function(x) {
  dims <- c(dim(x)[-3], 1)
  x <- (x[,,1, drop = FALSE] + x[,,2, drop = FALSE] + x[,,3, drop = FALSE])/3
  array(as.integer(x), dim = dims)
}

# reduces the scale of the image by skiping some pixels
downsample <- function(x, by = 2) {
  dims <- dim(x)
  x[seq(1, to = dims[1], by = by), seq(1, to = dims[2], by = by),]
}

# does both downsampling and grayscale.
preprocess <- function (x) {
  x %>%
    downsample(by = 4) %>%
    grayscale()
}
```

It's important to note that inside `grayscale` we transformed the resulting array to an integer array in order to save memmory.

## Experience

Now let's define an auxiliary class that will help us to keep the replay memory. Since we have to keep a large list of states, we need to take care to avoid copies. This is challeging in R because of the *copy-on-modify* beahvior.[@wickham2014advanced]. We implement an `R6` class manage storing and sampling from the replay memory.

```{r}
Experience <- R6::R6Class(
  "Experience",
  
  public = list(
    
    # class initialization
    initialize = function(max_size = 100000) {
      private$memory <- vector(mode = "list", max_size)
      private$max_size <- max_size
    },
    
    # adds transitions to the replay memory
    push = function(s_t, s_t1, terminal, action, reward) {
      
      private$memory[[private$i]] <- list(
        s_t = s_t,
        s_t1 = s_t1,
        terminal = as.logical(terminal),
        action = as.integer(action),
        reward = reward
      )
      
      if (private$i == private$max_size) {
        private$i <- 1L
        private$full <- TRUE
      } else {
        private$i <- private$i + 1 
      }
      
      invisible(TRUE)
    },
    
    # sample elements from the replay buffer
    sample = function(size = 32) {
      
      n <- ifelse(private$full, private$max_size, private$i - 1)
      ids <- sample.int(n, size)
      
      batch <- purrr::transpose(private$memory[ids])
      
      list(
        s_t = abind::abind(batch$s_t, along = 0.1),
        s_t1 = abind::abind(batch$s_t1, along = 0.1),
        terminal = unlist(batch$terminal),
        action = unlist(batch$action),
        reward = unlist(batch$reward)
      )
    }
    
  ),
  
  private = list(
    memory = NULL,
    i = 1,
    full = FALSE,
    max_size = NULL
  )
  
)
```

With this implementation we can create an instance of the `Experience` class with:

```{r}
experience <- Experience$new(max_size = 1000)
```

Then we can add transitions and sample from the replay buffer with:

```{r}
experience$push(s_t, s_t1, terminal, action, reward)
experience$sample(size = 32)
```

Note that the `experience` object is modified in place when pushing elements.

## Playing a game

We can now implement a function to play an *episode*, ie. a full match, of the game.
First we implemented an auxiliary function that will reset the environment and return a first valid state.

```{r}
reset_env <- function(env) {
  list(
    s_t = env$reset() %>%
      preprocess() %>%                     # do preprocessing
      abind::abind(., ., ., ., along = 3), # stack the first frame 4 times
    terminal = FALSE
  )
}
```

Now we will implement the `play_episode` function. Most of the logic of the DQN 
algorithm is implemented here. It's inputs are:

* `env`: an environment created by `gym$make`.
* `epsilon`: a numeric value (between 0 and 1) giving the probability of performing a random action. If 1 will perform all actions randomly and if 0, all actions will be chosen by the model.
* `models`: the models (we will detail in the next section)
* `experience`: an instance of the `Experience` class defined in the previous section.
* `train`: a boolean indicating if we will perform (or not) a training step. (we will also detail it later).
* `render`: a boolean indicating if you want to render the game. We recommend disabling it since it may slow down training.

The returned value of `play_episode` is the score attended by the algorithm.

<aside>
It's important to be familiar with the `%<-%` operator from the [zeallot package](https://github.com/r-lib/zeallot). The `%<-%` operator unpacks a list of values while assigning, for example:

```{r}
c(x, y) %<-% list(0, 1)
#> x
#[1] 0
#> y
#[1] 1
```
</aside>


```{r}
play_episode <- function(env, epsilon, models, experience, train, render = FALSE) {
  
  # resets the envinronment to the initial state
  c(s_t, terminal) %<-% reset_env(env)
  score <- 0 
  # while loop until the game ends - lost all 5 lifes or attended the maximum 
  # number of 10000 frames.
  while(!terminal) {
    
    # this if-else block will choose if the action performed will be random or
    # if the will use the model to choose the best action. 
    # the epsilon parameter controls the exploration/exploitation trade-off
    if (runif(1) < epsilon) {
      action <- env$action_space$sample()
    } else {
      score_predictions <- predict(
        models$score_model, 
        abind::abind(s_t, along = 0.1) # expand the first dimension 
      )
      action <- which.max(score_predictions) - 1L
    }
    
    # performs the selected action on the environment
    c(frame, reward, terminal, lives) %<-% env$step(action)
    if (render) env$render() # render the game
    
    # stacks last 3 frames from the previous state with the frame 
    # obtained at this timestep.
    s_t1 <- abind::abind(preprocess(frame), s_t[,,1:3], along = 3)
    
    # stores the transiion in the experience buffer.
    experience$push(
      s_t = s_t,           # previous state
      s_t1 = s_t1,         # new state
      terminal = terminal, # indicates if the game ended
      action = action,     # stores the performed action
      reward = reward      # the reward obtained by performing the action
    )
    
    # performs a train step (if selected)
    if (train) 
      train_step(models, experience, env$action_space$n)
    
    # update state and score
    s_t <- s_t1
    score <- score + reward
  }
  
  score
}
```

Since `experience` and `models` are modified in place we don't need to return them to persist their values. Note that unlike many other R functions `play_episode` has lot's of side-effects.

## Model

As discussed in the Deep Q-Learning section, the model we are going to implement is a convolutional neural network that will have 4 previous stacked frames as input to predict the future expected reward for each possible action. 

The implementation problem is that at training time, we only have the reward obtained by performed the selected action. This can be solved by masking the other values as we will see in the `build_model` function below.

```{r}
build_model <- function(input_shape, n_actions = 4) {
  
  # input layer
  input <- layer_input(input_shape)
  
  # creates the output layer.
  # gives a vector of `n_actions` elements with the expected reward of performing
  # each action
  output <- input %>%
    layer_lambda(function(x) {x/255}) %>%
    layer_conv_2d(
      filters = 16, 
      kernel_size = c(8,8), 
      strides = c(4,4), 
      activation = "relu"
    ) %>%
    layer_conv_2d(
      filters = 32, 
      kernel_size = c(4,4), 
      strides = c(2,2), 
      activation = "relu"
    ) %>%
    layer_flatten() %>%
    layer_dense(256, activation = "relu") %>%
    layer_dense(n_actions)
  
  # creates what we called the `score_model`;
  # predict(score_model, x = x) will return the expected reward for each possible
  # action. 
  score_model <- keras_model(input, output)
  
  # input layer for the action. The action is encoded as a one-hot vector.
  action <- layer_input(shape = n_actions)
  
  # get the expected reward by performing `action`.
  expected_reward <- layer_multiply(list(output, action)) %>%
   layer_lambda(f = function(x) k_sum(x, axis = 2, keepdims = TRUE))
  
  # creates the `action_model`. As input this model needs the previous observed
  # state and the selection. It will output the expected reward of performing the 
  # `action`.
  action_model <- keras_model(list(input, action), expected_reward)
  
  # we compile the action model chossing the Mean Squared Error loss and 
  # the Adam optimizer with learning_rate = 0.0001.
  action_model %>%
    compile(
      loss = "mse",
      optimizer = keras::optimizer_adam(lr = 0.0001)
    )
  
  # Finally, we return both models in a list.
  list(
    score_model = score_model,
    action_model = action_model
  )
}
```

The most important thing to note here is that `action_model` and `score_model` share all weights. This is important because we will not directly train the `score_model`. Instead we will train the `action_model` using gradient descent and use the `score_model` just to get predictions of rewards for each action.

Now we will define a function that will perform a step of the gradient descent algorithm to update the models weights.

```{r}
train_step <- function(models, experience, n_actions) {

  # sampling a minibatch of size 32 from the experience buffer
  c(s_t, s_t1, terminal, action, reward) %<-% experience$sample(32)
  
  # calculating the expected reward for s_t1
  expected_reward <- predict(models$score_model, s_t1) %>% apply(1, max)
  
  # calculating the Q value for state_t
  y <- reward + (!terminal)*gamma*expected_reward
  
  # transforming the action to a one-hot vector
  action <- to_categorical(action, n_actions)
  
  # perform the weights update with gradient descent
  loss <- train_on_batch(
    models$action_model, 
    x = list(s_t, action),
    y = y
  )
  
  # returns the obtained loss invisibly
  invisible(loss)
}
```

## Final loop

Now that we have all auxiliary functions we are ready to implement the final loop. First let's define all the hyperparameters that will control the training phase.

```{r}
observe <- 500 # number of episodes to observe before starting training
explore <- 4000 # nuber of episodes to do exploration
episodes <- 5000 # total number of episodes
replay_memory <- 100000 # number of transitions to remember
environment_name <- "BreakoutDeterministic-v4" # name of the gym environment 
initial_epsilon <- 1 # initial probability of choosing a random action
final_epsilon <- 0.1 # after `explore` iterations what should be the probability of choosing a rondom action
gamma <- 0.99 # gamma from q-learning
input_shape <- c(53, 40, 4) # size of the pre-processed array
```

Now let's start our objects:

```{r}
env <- gym$make(environment_name)
experience <- Experience$new(replay_memory)
history <- rep(NA, episodes)
models <- build_model(input_shape, env$action_space$n)
epsilon <- 1L
```

And run the final loop:

```{r}
for (i in seq_len(episodes)) {
  
  if (i >= observe & i < (observe + explore)) 
    epsilon <- epsilon - (initial_epsilon/final_epsilon) / explore
  
  if (i >= observe)
    train <- TRUE
  else 
    train <- FALSE
  
  time <- system.time({
    history[i] <- play_episode(env, epsilon, models, experience, train)  
  })
  
  last100 <- tail(history[!is.na(history)], 100) %>% mean() %>% round(2)
  time <- round(time[3], 2)
  
  # save model weights
  if(i %% 500 == 0)
    save_model_weights_hdf5(
      models$score_model, 
      "model_weights.hdf5", 
      overwrite = TRUE
    )
  
  cat(glue::glue("Episode: {i} | Score: {history[i]} | Mean: {last100} | Elapsed: {time} sec"), "\n")
}
```

Note, this loop can take a lot of time. It takes ~24h in a MacBook Air.


























