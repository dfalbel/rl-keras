---
title: "Playing Atari Breakout with Keras and R"
description: |
  In this article we will implement the famous paper from DeepMind: *Playing Atari 
  with Deep Reinforcement Learning* in R using Keras. This paper introduced the 
  first deep learning model that successfully learned control policies directly 
  from high-dimensional sensory input using reinforcement learning.
author:
  - name: Daniel Falbel 
    url: https://github.com/dfalbel
    affiliation: Curso-R
    affiliation_url: http://curso-r.com
  - name: Fernando Barscevicius
    url: https://github.com/feebarscevicius
    affiliation: Marketdata
    affiliation_url: http://marketdata.com.br
date: 07-17-2018
creative_commons: CC BY
repository_url: https://github.com/dfalbel/rl-keras
output: 
  radix::radix_article:
    self_contained: false
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

In this article we will implement the famous paper from DeepMind: [*Playing Atari with Deep Reinforcement Learning*](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) in R using Keras. This paper introduced the first deep learning model that successfully learned control policies directly from high-dimensional sensory input using reinforcement learning.

The DeepMinds's algorithm is trained on raw pixels from Atari games and estimates future rewards for each possible action. The model is a convolutional neural network trained with a variant of the [Q-Learning](https://en.wikipedia.org/wiki/Q-learning) algorithm, which we will attempt to explain during this article.

The code was not published with the paper, but the results were reproduced and published by severeal people on the internet. Particularly, [this post](https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html) and the [referenced implementation](https://github.com/yenchenlin/DeepLearningFlappyBird) were very useful when writing this article. Also, [this article](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26) from [Becoming Human](https://becominghuman.ai) was very inspiring.

We only tested our implementation for the Atari Breakout game, but it should also work for other Atari games.

# Reinforcement Learning

Reinforcement learning is an area of Machine Learning that has been gaining a lot of attention recently. A good definition comes from Sutton and Barto's book *Reinforcement Learning: An Introduction*:

> Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.

Reinforcement learning is being used to solve many problems in robotics, delivery management or in the finance sector.

## Q-Learning

Q-learning, [as defined by it's creator](https://link.springer.com/content/pdf/10.1007%2FBF00992698.pdf), *is a form of model-free reinforcement learning. It can also be viewed as a method of asynchronous dynamic programming (DP). It provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains.*

Put in simple terms, Q-learning is an algorithm that allows for the selection of the best possible action to perform given the current state of the environment. It works by assigning each state an estimated value **(Q-value)**. Then, by interacting with the state and receiveing a reward, the estimated value is updated. We can write this Q-value as $\mathbf{Q(s, a)}$ and the updates are given by $\mathbf{Q(s, a) = Q(s, a) + \alpha(R(s) + γmax_{a'}Q(s_0, a_0) − Q(s, a))}$ (more information about the mathematics and the workings of the Q-learning algorithm can be found [here](http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/)).

As we have to keep track of the Q-values for every state, storing and updating them in a Q-table, Q-learning can become very ineffective for bigger environments. That's where Deep Q-Learning comes into play, substituting the enormous Q-table for a lighter Neural Network.

## Deep Q-Learning

### Frame stacking

### Experience Replay

# Implementation in R

Now we will start implementing the Deep Q-Learning algorithm in R. But first, we need a game emulator. That's where [OpenAI gym](https://gym.openai.com) comes handy. 

## Gym

> Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.

The `gym` framework is available to python and we will use reticulate to run it from R. To install it, you can run:

```{r}
library(reticulate)
py_install("gym[atari]", envname = "r-tensorflow")
```

You must be careful to install it to the same environment that Keras and TensorFlow are installed since we will need to load all of them in the same session. Gym has lot's of dependencies that are not installed by default, to install the Atari dependencies that we are going to use we used `gym[atari]` in the pip package name.

Now that we have Gym installed we can "make" an environment and run some random actions to understand how it works.
We are going to use an environment called BreakoutDeterministic-v4. It differs from the canonical Breakout-v0 in two ways related noise added when performing actions:

* It doesn't repeat the performed action for more randomly chosen number of frames.
* It doesn't randomly repeat the previous action.

These differences make BreakoutDeterministic-v4 easier to learn than Breakout-v0.

```{r}
gym <- import("gym")
env <- gym$make("BreakoutDeterministic-v4")
initial_state <- env$reset()
```

Calling `env$reset()` will return a 3d array with the initial frame of the game.
We can render it by calliing `env$render()`.

With Gym we can also perform actions in the game by using the method `step`. We use the argument `action` to tell the environment which action to perform. Each different action is represented by an integer value. 

```{r}
result <- env$step(action = 0L)
```

The output of running a `step` in the environment is the next state of the game. The sate is represented by a list with the following structure:

```
List of 4
 $ : int [1:210, 1:160, 1:3] 0 0 0 0 0 0 0 0 0 0 ...
 $ : num 0
 $ : logi FALSE
 $ :List of 1
  ..$ ale.lives: int 5
```

The first element is an RGB array of the next frame, the second element is the reward received by performing the action and the thir is a boolean value indicating if the game has ended. The fourth element can contain other informations about the game, in this case, it returns the number of lives.

We can get the meaning of each action with `env.unwrapped.get_action_meanings()`. In our particular case, ie. the Atari Breakout game, there are 4 possible actions and the following table maps actions to corresponding integer values.

```{r, eval=TRUE, echo=FALSE}
knitr::kable(data.frame(
  "Action" = c("NOOP (Do nothing)", "FIRE (Get the ball)", "RIGHT", "LEFT" ),
  "Integer" = c(0, 1, 2, 3)
))
```

## Pre-processing

Now that we know about Gym, let's define some functions to preprocess the state before saving it to our experience memory. We will transform each frame to grayscale since the colors doesn't matter in this game and then we will rescale the image to save RAM memory.

We implemented 3 functions to do this:

```{r}
# transforms the input image to grayscale
grayscale <- function(x) {
  dims <- c(dim(x)[-3], 1)
  x <- (x[,,1, drop = FALSE] + x[,,2, drop = FALSE] + x[,,3, drop = FALSE])/3
  array(as.integer(x), dim = dims)
}

# reduces the scale of the image by skiping some pixels
downsample <- function(x, by = 2) {
  dims <- dim(x)
  x[seq(1, to = dims[1], by = by), seq(1, to = dims[2], by = by),]
}

# does both downsampling and grayscale.
preprocess <- function (x) {
  x %>%
    downsample(by = 4) %>%
    grayscale()
}
```

It's important to note that inside `grayscale` we transformed the resulting array to an integer array in order to save memmory.

## Experience

Now let's define an auxiliary class that will help us to keep the replay memory. Since we have to keep a large list of states, we need to take care to avoid copies. This is challeging in R because of the *copy-on-modify* beahvior.[@wickham2014advanced]. We implement an `R6` class manage storing and sampling from the replay memory.

```{r}
Experience <- R6::R6Class(
  "Experience",
  
  public = list(
    
    # class initialization
    initialize = function(max_size = 100000) {
      private$memory <- vector(mode = "list", max_size)
      private$max_size <- max_size
    },
    
    # adds transitions to the replay memory
    push = function(s_t, s_t1, terminal, action, reward) {
      
      private$memory[[private$i]] <- list(
        s_t = s_t,
        s_t1 = s_t1,
        terminal = as.logical(terminal),
        action = as.integer(action),
        reward = reward
      )
      
      if (private$i == private$max_size) {
        private$i <- 1L
        private$full <- TRUE
      } else {
        private$i <- private$i + 1 
      }
      
      invisible(TRUE)
    },
    
    # sample elements from the replay buffer
    sample = function(size = 32) {
      
      n <- ifelse(private$full, private$max_size, private$i - 1)
      ids <- sample.int(n, size)
      
      batch <- purrr::transpose(private$memory[ids])
      
      list(
        s_t = abind::abind(batch$s_t, along = 0.1),
        s_t1 = abind::abind(batch$s_t1, along = 0.1),
        terminal = unlist(batch$terminal),
        action = unlist(batch$action),
        reward = unlist(batch$reward)
      )
    }
    
  ),
  
  private = list(
    memory = NULL,
    i = 1,
    full = FALSE,
    max_size = NULL
  )
  
)
```

With this implementation we can create an instance of the `Experience` class with:

```{r}
experience <- Experience$new(max_size = 1000)
```

Then we can add transitions and sample from the replay buffer with:

```{r}
experience$push(s_t, s_t1, terminal, action, reward)
experience$sample(size = 32)
```

Note that the `experience` object is modified in place when pushing elements.

## Playing a game

We can now implement a function to play an *episode*, ie. a full match, of the game.
First we implemented an auxiliary function that will reset the environment and return a first valid state.

```{r}
reset_env <- function(env) {
  list(
    s_t = env$reset() %>%
      preprocess() %>%                     # do preprocessing
      abind::abind(., ., ., ., along = 3), # stack the first frame 4 times
    terminal = FALSE
  )
}
```

Now we will implement the `play_episode` function. Most of the logic of the DQN 
algorithm is implemented here. It's inputs are:

* `env`: an environment created by `gym$make`.
* `epsilon`: a numeric value (between 0 and 1) giving the probability of performing a random action. If 1 will perform all actions randomly and if 0, all actions will be chosen by the model.
* `models`: the models (we will detail in the next section)
* `experience`: an instance of the `Experience` class defined in the previous section.
* `train`: a boolean indicating if we will perform (or not) a training step. (we will also detail it later).
* `render`: a boolean indicating if you want to render the game. We recommend disabling it since it may slow down training.

The returned value of `play_episode` is the score attended by the algorithm.

<aside>
It's important to be familiar with the `%<-%` operator from the [zeallot package](https://github.com/r-lib/zeallot). The `%<-%` operator unpacks a list of values while assigning, for example:

```{r}
c(x, y) %<-% list(0, 1)
#> x
#[1] 0
#> y
#[1] 1
```
</aside>


```{r}
play_episode <- function(env, epsilon, models, experience, train, render = FALSE) {
  
  # resets the envinronment to the initial state
  c(s_t, terminal) %<-% reset_env(env)
  score <- 0 
  # while loop until the game ends - lost all 5 lifes or attended the maximum 
  # number of 10000 frames.
  while(!terminal) {
    
    # this if-else block will choose if the action performed will be random or
    # if the will use the model to choose the best action. 
    # the epsilon parameter controls the exploration/exploitation trade-off
    if (runif(1) < epsilon) {
      action <- env$action_space$sample()
    } else {
      score_predictions <- predict(models$score_model, abind(s_t, along = 0.1))
      action <- which.max(score_predictions) - 1L
    }
    
    # performs the selected action on the environment
    c(frame, reward, terminal, lives) %<-% env$step(action)
    if (render) env$render() # render the game
    
    # stacks last 3 frames from the previous state with the frame 
    # obtained at this timestep.
    s_t1 <- abind::abind(preprocess(frame), s_t[,,1:3], along = 3)
    
    # stores the transiion in the experience buffer.
    experience$push(
      s_t = s_t,           # previous state
      s_t1 = s_t1,         # new state
      terminal = terminal, # indicates if the game ended
      action = action,     # stores the performed action
      reward = reward      # the reward obtained by performing the action
    )
    
    # performs a train step (if selected)
    if (train) 
      train_step(models, experience, env$action_space$n)
    
    # update state and score
    s_t <- s_t1
    score <- score + reward
  }
  
  score
}
```

Since `experience` and `models` are modified in place we don't need to return them to persist their values. Note that unlike many other R functions `play_episode` has lot's of side-effects.

























