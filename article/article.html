<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="radix" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Playing Atari Breakout with Keras and R</title>
  
  <meta property="description" itemprop="description" content="In this article we will implement the famous paper from DeepMind: *Playing Atari &#10;with Deep Reinforcement Learning* in R using Keras. This paper introduced the &#10;first deep learning model that successfully learned control policies directly &#10;from high-dimensional sensory input using reinforcement learning.&#10;"/>
  
  <link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2018-07-17"/>
  <meta property="article:created" itemprop="dateCreated" content="2018-07-17"/>
  <meta name="article:author" content="Daniel Falbel"/>
  <meta name="article:author" content="Fernando Barscevicius"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Playing Atari Breakout with Keras and R"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="In this article we will implement the famous paper from DeepMind: *Playing Atari &#10;with Deep Reinforcement Learning* in R using Keras. This paper introduced the &#10;first deep learning model that successfully learned control policies directly &#10;from high-dimensional sensory input using reinforcement learning.&#10;"/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Playing Atari Breakout with Keras and R"/>
  <meta property="twitter:description" content="In this article we will implement the famous paper from DeepMind: *Playing Atari &#10;with Deep Reinforcement Learning* in R using Keras. This paper introduced the &#10;first deep learning model that successfully learned control policies directly &#10;from high-dimensional sensory input using reinforcement learning.&#10;"/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Advanced r;citation_publication_date=2014;citation_publisher=Taylor &amp; Francis;citation_author=H. Wickham"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","creative_commons","repository_url","output","bibliography"]}},"value":[{"type":"character","attributes":{},"value":["Playing Atari Breakout with Keras and R"]},{"type":"character","attributes":{},"value":["In this article we will implement the famous paper from DeepMind: *Playing Atari \nwith Deep Reinforcement Learning* in R using Keras. This paper introduced the \nfirst deep learning model that successfully learned control policies directly \nfrom high-dimensional sensory input using reinforcement learning.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Daniel Falbel"]},{"type":"character","attributes":{},"value":["https://github.com/dfalbel"]},{"type":"character","attributes":{},"value":["Curso-R"]},{"type":"character","attributes":{},"value":["http://curso-r.com"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation","affiliation_url"]}},"value":[{"type":"character","attributes":{},"value":["Fernando Barscevicius"]},{"type":"character","attributes":{},"value":["https://github.com/feebarscevicius"]},{"type":"character","attributes":{},"value":["Marketdata"]},{"type":"character","attributes":{},"value":["http://marketdata.com.br"]}]}]},{"type":"character","attributes":{},"value":["07-17-2018"]},{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["https://github.com/dfalbel/rl-keras"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["radix::radix_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["biblio.bib"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["article_files/bowser-1.9.3/bowser.min.js","article_files/distill-2.2.21/template.v2.js","article_files/jquery-1.11.3/jquery.min.js","article_files/webcomponents-2.0.0/webcomponents.js","biblio.bib"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#fn1>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.text();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.text(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        if ($.inArray(language, ["r", "cpp", "c", "java"]) != -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      $(this).children().not('d-code, pre.text-output')
        .wrap($('<div class="' + layout + '"></div>'));
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      $(this).find('img, .html-widget').css('width', '100%');
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="article_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="article_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="article_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="article_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Playing Atari Breakout with Keras and R","description":"In this article we will implement the famous paper from DeepMind: *Playing Atari \nwith Deep Reinforcement Learning* in R using Keras. This paper introduced the \nfirst deep learning model that successfully learned control policies directly \nfrom high-dimensional sensory input using reinforcement learning.\n","authors":[{"author":"Daniel Falbel","authorURL":"https://github.com/dfalbel","affiliation":"Curso-R","affiliationURL":"http://curso-r.com"},{"author":"Fernando Barscevicius","authorURL":"https://github.com/feebarscevicius","affiliation":"Marketdata","affiliationURL":"http://marketdata.com.br"}],"publishedDate":"2018-07-17T00:00:00.000-03:00","citationText":"Falbel & Barscevicius, 2018"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Playing Atari Breakout with Keras and R</h1>
<p>In this article we will implement the famous paper from DeepMind: <em>Playing Atari with Deep Reinforcement Learning</em> in R using Keras. This paper introduced the first deep learning model that successfully learned control policies directly from high-dimensional sensory input using reinforcement learning.</p>
</div>

<div class="d-byline">
  Daniel Falbel <a href="https://github.com/dfalbel" class="uri">https://github.com/dfalbel</a> (Curso-R)<a href="http://curso-r.com" class="uri">http://curso-r.com</a>
  
,   Fernando Barscevicius <a href="https://github.com/feebarscevicius" class="uri">https://github.com/feebarscevicius</a> (Marketdata)<a href="http://marketdata.com.br" class="uri">http://marketdata.com.br</a>
  
<br/>07-17-2018
</div>

<div class="d-article">
<p>In this article we will implement the famous paper from DeepMind: <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"><em>Playing Atari with Deep Reinforcement Learning</em></a> in R using Keras. This paper introduced the first deep learning model that successfully learned control policies directly from high-dimensional sensory input using reinforcement learning.</p>
<p>The DeepMinds’s algorithm is trained on raw pixels from Atari games and estimates future rewards for each possible action. The model is a convolutional neural network trained with a variant of the <a href="https://en.wikipedia.org/wiki/Q-learning">Q-Learning</a> algorithm, which we will attempt to explain during this article.</p>
<p>The code was not published with the paper, but the results were reproduced and published by severeal people on the internet. Particularly, <a href="https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html">this post</a> and the <a href="https://github.com/yenchenlin/DeepLearningFlappyBird">referenced implementation</a> were very useful when writing this article. Also, <a href="https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26">this article</a> from <a href="https://becominghuman.ai">Becoming Human</a> was very inspiring.</p>
<p>We only tested our implementation for the Atari Breakout game, but it should also work for other Atari games.</p>
<h1 id="reinforcement-learning">Reinforcement Learning</h1>
<p>Reinforcement learning is an area of Machine Learning that has been gaining a lot of attention recently. A good definition comes from Sutton and Barto’s book <em>Reinforcement Learning: An Introduction</em>:</p>
<blockquote>
<p>Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.</p>
</blockquote>
<p>Reinforcement learning is being used to solve many problems in robotics, delivery management or in the finance sector.</p>
<h2 id="q-learning">Q-Learning</h2>
<p>Q-learning, <a href="https://link.springer.com/content/pdf/10.1007%2FBF00992698.pdf">as defined by it’s creator</a>, <em>is a form of model-free reinforcement learning. It can also be viewed as a method of asynchronous dynamic programming (DP). It provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains.</em></p>
<p>Put in simple terms, Q-learning is an algorithm that allows for the selection of the best possible action to perform given the current state of the environment. It works by assigning each state an estimated value <strong>(Q-value)</strong>. Then, by interacting with the state and receiveing a reward, the estimated value is updated. We can write this Q-value as <span class="math inline">\(\mathbf{Q(s, a)}\)</span> and the updates are given by <span class="math inline">\(\mathbf{Q(s, a) = Q(s, a) + \alpha(R(s) + γmax_{a&#39;}Q(s_0, a_0) − Q(s, a))}\)</span> (more information about the mathematics and the workings of the Q-learning algorithm can be found <a href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/">here</a>).</p>
<p>As we have to keep track of the Q-values for every state, storing and updating them in a Q-table, Q-learning can become very ineffective for bigger environments. That’s where Deep Q-Learning comes into play, substituting the enormous Q-table for a lighter Neural Network.</p>
<h2 id="deep-q-learning">Deep Q-Learning</h2>
<h3 id="frame-stacking">Frame stacking</h3>
<h3 id="experience-replay">Experience Replay</h3>
<h1 id="implementation-in-r">Implementation in R</h1>
<p>Now we will start implementing the Deep Q-Learning algorithm in R. But first, we need a game emulator. That’s where <a href="https://gym.openai.com">OpenAI gym</a> comes handy.</p>
<h2 id="gym">Gym</h2>
<blockquote>
<p>Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.</p>
</blockquote>
<p>The <code>gym</code> framework is available to python and we will use reticulate to run it from R. To install it, you can run:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
library(reticulate)
py_install(&quot;gym[atari]&quot;, envname = &quot;r-tensorflow&quot;)</code></pre>
</div>
<p>You must be careful to install it to the same environment that Keras and TensorFlow are installed since we will need to load all of them in the same session. Gym has lot’s of dependencies that are not installed by default, to install the Atari dependencies that we are going to use we used <code>gym[atari]</code> in the pip package name.</p>
<p>Now that we have Gym installed we can “make” an environment and run some random actions to understand how it works. We are going to use an environment called BreakoutDeterministic-v4. It differs from the canonical Breakout-v0 in two ways related noise added when performing actions:</p>
<ul>
<li>It doesn’t repeat the performed action for more randomly chosen number of frames.</li>
<li>It doesn’t randomly repeat the previous action.</li>
</ul>
<p>These differences make BreakoutDeterministic-v4 easier to learn than Breakout-v0.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
gym &lt;- import(&quot;gym&quot;)
env &lt;- gym$make(&quot;BreakoutDeterministic-v4&quot;)
initial_state &lt;- env$reset()</code></pre>
</div>
<p>Calling <code>env$reset()</code> will return a 3d array with the initial frame of the game. We can render it by calliing <code>env$render()</code>.</p>
<p>With Gym we can also perform actions in the game by using the method <code>step</code>. We use the argument <code>action</code> to tell the environment which action to perform. Each different action is represented by an integer value.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
result &lt;- env$step(action = 0L)</code></pre>
</div>
<p>The output of running a <code>step</code> in the environment is the next state of the game. The sate is represented by a list with the following structure:</p>
<pre><code>
List of 4
 $ : int [1:210, 1:160, 1:3] 0 0 0 0 0 0 0 0 0 0 ...
 $ : num 0
 $ : logi FALSE
 $ :List of 1
  ..$ ale.lives: int 5</code></pre>
<p>The first element is an RGB array of the next frame, the second element is the reward received by performing the action and the thir is a boolean value indicating if the game has ended. The fourth element can contain other informations about the game, in this case, it returns the number of lives.</p>
<p>We can get the meaning of each action with <code>env.unwrapped.get_action_meanings()</code>. In our particular case, ie. the Atari Breakout game, there are 4 possible actions and the following table maps actions to corresponding integer values.</p>
<div class="layout-chunk" data-layout="l-body">
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Action</th>
<th style="text-align: right;">Integer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">NOOP (Do nothing)</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">FIRE (Get the ball)</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RIGHT</td>
<td style="text-align: right;">2</td>
</tr>
<tr class="even">
<td style="text-align: left;">LEFT</td>
<td style="text-align: right;">3</td>
</tr>
</tbody>
</table>
</div>
<h2 id="pre-processing">Pre-processing</h2>
<p>Now that we know about Gym, let’s define some functions to preprocess the state before saving it to our experience memory. We will transform each frame to grayscale since the colors doesn’t matter in this game and then we will rescale the image to save RAM memory.</p>
<p>We implemented 3 functions to do this:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# transforms the input image to grayscale
grayscale &lt;- function(x) {
  dims &lt;- c(dim(x)[-3], 1)
  x &lt;- (x[,,1, drop = FALSE] + x[,,2, drop = FALSE] + x[,,3, drop = FALSE])/3
  array(as.integer(x), dim = dims)
}

# reduces the scale of the image by skiping some pixels
downsample &lt;- function(x, by = 2) {
  dims &lt;- dim(x)
  x[seq(1, to = dims[1], by = by), seq(1, to = dims[2], by = by),]
}

# does both downsampling and grayscale.
preprocess &lt;- function (x) {
  x %&gt;%
    downsample(by = 4) %&gt;%
    grayscale()
}</code></pre>
</div>
<p>It’s important to note that inside <code>grayscale</code> we transformed the resulting array to an integer array in order to save memmory.</p>
<h2 id="experience">Experience</h2>
<p>Now let’s define an auxiliary class that will help us to keep the replay memory. Since we have to keep a large list of states, we need to take care to avoid copies. This is challeging in R because of the <em>copy-on-modify</em> beahvior.<span class="citation" data-cites="wickham2014advanced">(Wickham <a href="#ref-wickham2014advanced">2014</a>)</span>. We implement an <code>R6</code> class manage storing and sampling from the replay memory.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
Experience &lt;- R6::R6Class(
  &quot;Experience&quot;,
  
  public = list(
    
    # class initialization
    initialize = function(max_size = 100000) {
      private$memory &lt;- vector(mode = &quot;list&quot;, max_size)
      private$max_size &lt;- max_size
    },
    
    # adds transitions to the replay memory
    push = function(s_t, s_t1, terminal, action, reward) {
      
      private$memory[[private$i]] &lt;- list(
        s_t = s_t,
        s_t1 = s_t1,
        terminal = as.logical(terminal),
        action = as.integer(action),
        reward = reward
      )
      
      if (private$i == private$max_size) {
        private$i &lt;- 1L
        private$full &lt;- TRUE
      } else {
        private$i &lt;- private$i + 1 
      }
      
      invisible(TRUE)
    },
    
    # sample elements from the replay buffer
    sample = function(size = 32) {
      
      n &lt;- ifelse(private$full, private$max_size, private$i - 1)
      ids &lt;- sample.int(n, size)
      
      batch &lt;- purrr::transpose(private$memory[ids])
      
      list(
        s_t = abind::abind(batch$s_t, along = 0.1),
        s_t1 = abind::abind(batch$s_t1, along = 0.1),
        terminal = unlist(batch$terminal),
        action = unlist(batch$action),
        reward = unlist(batch$reward)
      )
    }
    
  ),
  
  private = list(
    memory = NULL,
    i = 1,
    full = FALSE,
    max_size = NULL
  )
  
)</code></pre>
</div>
<p>With this implementation we can create an instance of the <code>Experience</code> class with:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
experience &lt;- Experience$new(max_size = 1000)</code></pre>
</div>
<p>Then we can add transitions and sample from the replay buffer with:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
experience$push(s_t, s_t1, terminal, action, reward)
experience$sample(size = 32)</code></pre>
</div>
<p>Note that the <code>experience</code> object is modified in place when pushing elements.</p>
<h2 id="playing-a-game">Playing a game</h2>
<p>We can now implement a function to play an <em>episode</em>, ie. a full match, of the game. First we implemented an auxiliary function that will reset the environment and return a first valid state.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
reset_env &lt;- function(env) {
  list(
    s_t = env$reset() %&gt;%
      preprocess() %&gt;%                     # do preprocessing
      abind::abind(., ., ., ., along = 3), # stack the first frame 4 times
    terminal = FALSE
  )
}</code></pre>
</div>
<p>Now we will implement the <code>play_episode</code> function. Most of the logic of the DQN algorithm is implemented here. It’s inputs are:</p>
<ul>
<li><code>env</code>: an environment created by <code>gym$make</code>.</li>
<li><code>epsilon</code>: a numeric value (between 0 and 1) giving the probability of performing a random action. If 1 will perform all actions randomly and if 0, all actions will be chosen by the model.</li>
<li><code>models</code>: the models (we will detail in the next section)</li>
<li><code>experience</code>: an instance of the <code>Experience</code> class defined in the previous section.</li>
<li><code>train</code>: a boolean indicating if we will perform (or not) a training step. (we will also detail it later).</li>
<li><code>render</code>: a boolean indicating if you want to render the game. We recommend disabling it since it may slow down training.</li>
</ul>
<p>The returned value of <code>play_episode</code> is the score attended by the algorithm.</p>
<aside>
<p>It’s important to be familiar with the <code>%&lt;-%</code> operator from the <a href="https://github.com/r-lib/zeallot">zeallot package</a>. The <code>%&lt;-%</code> operator unpacks a list of values while assigning, for example:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
c(x, y) %&lt;-% list(0, 1)
#&gt; x
#[1] 0
#&gt; y
#[1] 1</code></pre>
</div>
</aside>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
play_episode &lt;- function(env, epsilon, models, experience, train, render = FALSE) {
  
  # resets the envinronment to the initial state
  c(s_t, terminal) %&lt;-% reset_env(env)
  score &lt;- 0 
  # while loop until the game ends - lost all 5 lifes or attended the maximum 
  # number of 10000 frames.
  while(!terminal) {
    
    # this if-else block will choose if the action performed will be random or
    # if the will use the model to choose the best action. 
    # the epsilon parameter controls the exploration/exploitation trade-off
    if (runif(1) &lt; epsilon) {
      action &lt;- env$action_space$sample()
    } else {
      score_predictions &lt;- predict(models$score_model, abind(s_t, along = 0.1))
      action &lt;- which.max(score_predictions) - 1L
    }
    
    # performs the selected action on the environment
    c(frame, reward, terminal, lives) %&lt;-% env$step(action)
    if (render) env$render() # render the game
    
    # stacks last 3 frames from the previous state with the frame 
    # obtained at this timestep.
    s_t1 &lt;- abind::abind(preprocess(frame), s_t[,,1:3], along = 3)
    
    # stores the transiion in the experience buffer.
    experience$push(
      s_t = s_t,           # previous state
      s_t1 = s_t1,         # new state
      terminal = terminal, # indicates if the game ended
      action = action,     # stores the performed action
      reward = reward      # the reward obtained by performing the action
    )
    
    # performs a train step (if selected)
    if (train) 
      train_step(models, experience, env$action_space$n)
    
    # update state and score
    s_t &lt;- s_t1
    score &lt;- score + reward
  }
  
  score
}</code></pre>
</div>
<p>Since <code>experience</code> and <code>models</code> are modified in place we don’t need to return them to persist their values. Note that unlike many other R functions <code>play_episode</code> has lot’s of side-effects.</p>
<div id="refs" class="references">
<div id="ref-wickham2014advanced">
<p>Wickham, H. 2014. <em>Advanced R</em>. Chapman &amp; Hall/Crc the R Series. Taylor &amp; Francis. <a href="https://books.google.com.br/books?id=PFHFNAEACAAJ" class="uri">https://books.google.com.br/books?id=PFHFNAEACAAJ</a>.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="updates-and-corrections">Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/dfalbel/rl-keras/issues/new">create an issue</a> on the source repository.</p>
<h3 id="reuse">Reuse</h3>
<p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. Source code is available at <a href="https://github.com/dfalbel/rl-keras">https://github.com/dfalbel/rl-keras</a>, unless otherwise noted. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
</div>
<script id="distill-bibliography" type="text/bibtex">
@book{wickham2014advanced,
  title={Advanced R},
  author={Wickham, H.},
  isbn={9781466586963},
  lccn={2012278240},
  series={Chapman \& Hall/CRC The R Series},
  url={https://books.google.com.br/books?id=PFHFNAEACAAJ},
  year={2014},
  publisher={Taylor \& Francis}
}
</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
